{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pd.read_csv(\"../data/dataset_fustion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train & test 데이터로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "train_data, test_data = train_test_split(data_path, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document 열의 중복 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 수 : 23923\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 수 :',len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오다', '이렇다', '것', '도', '영화', '라고', '차라리', '뮤직비디오', '를', '만들다', '게', '나다', '뻔']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "okt.morphs('와 이런 것도 영화라고 차라리 뮤직비디오를 만드는 게 나을 뻔', stem = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "20000\n",
      "22000\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "#돌아가는지 확인용...\n",
    "cnt=-1 \n",
    "for sentence in train_data['document']: \n",
    "    cnt = cnt +1 \n",
    "    if cnt%2000==0:\n",
    "        print(cnt)\n",
    "    temp_X = [] \n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거 \n",
    "    X_train.append(temp_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 38000\n",
    "tokenizer = Tokenizer(num_words = max_words) \n",
    "tokenizer.fit_on_texts(X_train) \n",
    "X_train = tokenizer.texts_to_sequences(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최대 길이 :  113\n",
      "문장의 평균 길이 :  13.783137566358734\n"
     ]
    }
   ],
   "source": [
    "print(\"문장의 최대 길이 : \", max(len(l) for l in X_train)) \n",
    "print(\"문장의 평균 길이 : \", sum(map(len, X_train))/ len(X_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "#원핫인코딩\n",
    "for i in range(len(train_data['label'])): \n",
    "    if train_data['label'].iloc[i] == 1: \n",
    "        y_train.append([0, 0, 1]) \n",
    "    elif train_data['label'].iloc[i] == 0:\n",
    "        y_train.append([0, 1, 0]) \n",
    "    elif train_data['label'].iloc[i] == -1:\n",
    "        y_train.append([1, 0, 0])\n",
    "\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.1, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 35 # 전체 데이터의 길이를 15로 맞춘다 \n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, LSTM, Dropout\n",
    "from keras.models import Sequential \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.layers import BatchNormalization\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words,128))\n",
    "model.add(LSTM(64, return_sequences = True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6)) # 드롭아웃 추가. 비율은 60%\n",
    "model.add(LSTM(32, return_sequences = False))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2)) # 드롭아웃 추가. 비율은 20%\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1)) # 드롭아웃 추가. 비율은 20%\n",
    "model.add(Dense(9, activation='relu')) \n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model2.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "216/216 [==============================] - ETA: 0s - loss: 0.4564 - acc: 0.6601\n",
      "Epoch 1: val_acc improved from -inf to 0.54033, saving model to best_model2.h5\n",
      "216/216 [==============================] - 11s 42ms/step - loss: 0.4564 - acc: 0.6601 - val_loss: 0.4846 - val_acc: 0.5403\n",
      "Epoch 2/15\n",
      "  3/216 [..............................] - ETA: 8s - loss: 0.3455 - acc: 0.7600"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/conda_env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/216 [============================>.] - ETA: 0s - loss: 0.3009 - acc: 0.7974\n",
      "Epoch 2: val_acc improved from 0.54033 to 0.73422, saving model to best_model2.h5\n",
      "216/216 [==============================] - 9s 41ms/step - loss: 0.3010 - acc: 0.7972 - val_loss: 0.3528 - val_acc: 0.7342\n",
      "Epoch 3/15\n",
      "215/216 [============================>.] - ETA: 0s - loss: 0.2464 - acc: 0.8427\n",
      "Epoch 3: val_acc improved from 0.73422 to 0.76097, saving model to best_model2.h5\n",
      "216/216 [==============================] - 9s 43ms/step - loss: 0.2463 - acc: 0.8428 - val_loss: 0.3340 - val_acc: 0.7610\n",
      "Epoch 4/15\n",
      "216/216 [==============================] - ETA: 0s - loss: 0.2080 - acc: 0.8756\n",
      "Epoch 4: val_acc improved from 0.76097 to 0.76264, saving model to best_model2.h5\n",
      "216/216 [==============================] - 9s 43ms/step - loss: 0.2080 - acc: 0.8756 - val_loss: 0.3651 - val_acc: 0.7626\n",
      "Epoch 5/15\n",
      "215/216 [============================>.] - ETA: 0s - loss: 0.1803 - acc: 0.8948\n",
      "Epoch 5: val_acc improved from 0.76264 to 0.76473, saving model to best_model2.h5\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1802 - acc: 0.8949 - val_loss: 0.4161 - val_acc: 0.7647\n",
      "Epoch 6/15\n",
      "215/216 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9129\n",
      "Epoch 6: val_acc did not improve from 0.76473\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1539 - acc: 0.9130 - val_loss: 0.4619 - val_acc: 0.7489\n",
      "Epoch 7/15\n",
      "215/216 [============================>.] - ETA: 0s - loss: 0.1328 - acc: 0.9272\n",
      "Epoch 7: val_acc did not improve from 0.76473\n",
      "216/216 [==============================] - 9s 42ms/step - loss: 0.1328 - acc: 0.9272 - val_loss: 0.4656 - val_acc: 0.7401\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, batch_size=100, epochs=15, callbacks=[es, mc], validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emotion_tokenizer.pickle', 'wb') as handle:\n",
    "     pickle.dump(tokenizer, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 1s 5ms/step - loss: 0.4161 - acc: 0.7647\n",
      "\n",
      " 테스트 정확도: 0.7647\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('emotion_model.h5')\n",
    "\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
